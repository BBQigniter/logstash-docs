[[plugins-filters-jdbc_streaming]]
=== jdbc_streaming

* Version: 1.0.0
* Released on: March 27, 2017
* https://github.com/logstash-plugins/logstash-filter-jdbc_streaming/blob/master/CHANGELOG.md#100[Changelog]


==== Installation

For plugins not bundled by default, it is easy to install by running `bin/logstash-plugin install logstash-filter-jdbc_streaming`. See <<working-with-plugins>> for more details.


==== Getting Help

For questions about the plugin, open a topic in the http://discuss.elastic.co[Discuss] forums. For bugs or feature requests, open an issue in https://github.com/elastic/logstash[Github].
For the list of Elastic supported plugins, please consult the https://www.elastic.co/support/matrix#show_logstash_plugins[Elastic Support Matrix].

==== Description

Enrich Logstash events with your database data. This filter executes a SQL query and stores the result set in the field
specified as `target`. It will cache the results locally in an LRU cache with a configurable expiry time.

For example you can load a row based on an id from an event

[source,ruby]
filter {
  jdbc_streaming {
    jdbc_driver_library => "/path/to/mysql-connector-java-5.1.34-bin.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => ""jdbc:mysql://localhost:3306/mydatabase"
    jdbc_user => "me"
    jdbc_password => "secret"
    statement => "select * from WORLD.COUNTRY WHERE Code = :code"
    parameters => { "code" => "country_code"}
    target => "country_details"
  }
}


&nbsp;

==== Synopsis

This plugin supports the following configuration options:

Required configuration options:

[source,json]
--------------------------
jdbc_streaming {
    jdbc_connection_string => ...
    jdbc_driver_class => ...
    statement => ...
    target => ...
}
--------------------------



Available configuration options:

[cols="<,<,<",options="header",]
|=======================================================================
|Setting |Input type|Required
| <<plugins-filters-jdbc_streaming-add_field>> |<<hash,hash>>|No
| <<plugins-filters-jdbc_streaming-add_tag>> |<<array,array>>|No
| <<plugins-filters-jdbc_streaming-cache_expiration>> |<<number,number>>|No
| <<plugins-filters-jdbc_streaming-cache_size>> |<<number,number>>|No
| <<plugins-filters-jdbc_streaming-default_hash>> |<<hash,hash>>|No
| <<plugins-filters-jdbc_streaming-enable_metric>> |<<boolean,boolean>>|No
| <<plugins-filters-jdbc_streaming-id>> |<<string,string>>|No
| <<plugins-filters-jdbc_streaming-jdbc_connection_string>> |<<string,string>>|Yes
| <<plugins-filters-jdbc_streaming-jdbc_driver_class>> |<<string,string>>|Yes
| <<plugins-filters-jdbc_streaming-jdbc_driver_library>> |a valid filesystem path|No
| <<plugins-filters-jdbc_streaming-jdbc_password>> |<<password,password>>|No
| <<plugins-filters-jdbc_streaming-jdbc_user>> |<<string,string>>|No
| <<plugins-filters-jdbc_streaming-jdbc_validate_connection>> |<<boolean,boolean>>|No
| <<plugins-filters-jdbc_streaming-jdbc_validation_timeout>> |<<number,number>>|No
| <<plugins-filters-jdbc_streaming-parameters>> |<<hash,hash>>|No
| <<plugins-filters-jdbc_streaming-periodic_flush>> |<<boolean,boolean>>|No
| <<plugins-filters-jdbc_streaming-remove_field>> |<<array,array>>|No
| <<plugins-filters-jdbc_streaming-remove_tag>> |<<array,array>>|No
| <<plugins-filters-jdbc_streaming-statement>> |<<string,string>>|Yes
| <<plugins-filters-jdbc_streaming-tag_on_default_use>> |<<array,array>>|No
| <<plugins-filters-jdbc_streaming-tag_on_failure>> |<<array,array>>|No
| <<plugins-filters-jdbc_streaming-target>> |<<string,string>>|Yes
| <<plugins-filters-jdbc_streaming-use_cache>> |<<boolean,boolean>>|No
|=======================================================================


==== Details

&nbsp;

[[plugins-filters-jdbc_streaming-add_field]]
===== `add_field` 

  * Value type is <<hash,hash>>
  * Default value is `{}`

If this filter is successful, add any arbitrary fields to this event.
Field names can be dynamic and include parts of the event using the `%{field}`.

Example:
[source,ruby]
    filter {
      jdbc_streaming {
        add_field => { "foo_%{somefield}" => "Hello world, from %{host}" }
      }
    }
[source,ruby]
    # You can also add multiple fields at once:
    filter {
      jdbc_streaming {
        add_field => {
          "foo_%{somefield}" => "Hello world, from %{host}"
          "new_field" => "new_static_value"
        }
      }
    }

If the event has field `"somefield" == "hello"` this filter, on success,
would add field `foo_hello` if it is present, with the
value above and the `%{host}` piece replaced with that value from the
event. The second example would also add a hardcoded field.

[[plugins-filters-jdbc_streaming-add_tag]]
===== `add_tag` 

  * Value type is <<array,array>>
  * Default value is `[]`

If this filter is successful, add arbitrary tags to the event.
Tags can be dynamic and include parts of the event using the `%{field}`
syntax.

Example:
[source,ruby]
    filter {
      jdbc_streaming {
        add_tag => [ "foo_%{somefield}" ]
      }
    }
[source,ruby]
    # You can also add multiple tags at once:
    filter {
      jdbc_streaming {
        add_tag => [ "foo_%{somefield}", "taggedy_tag"]
      }
    }

If the event has field `"somefield" == "hello"` this filter, on success,
would add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).

[[plugins-filters-jdbc_streaming-cache_expiration]]
===== `cache_expiration` 

  * Value type is <<number,number>>
  * Default value is `5.0`

The minimum number of seconds any entry should remain in the cache, defaults to 5 seconds
A numeric value, you can use decimals for example `{ "cache_expiration" => 0.25 }`
If there are transient jdbc errors the cache will store empty results for a given
parameter set and bypass the jbdc lookup, this merges the default_hash into the event, until
the cache entry expires, then the jdbc lookup will be tried again for the same parameters
Conversely, while the cache contains valid results any external problem that would cause
jdbc errors, will not be noticed for the cache_expiration period.

[[plugins-filters-jdbc_streaming-cache_size]]
===== `cache_size` 

  * Value type is <<number,number>>
  * Default value is `500`

The maximum number of cache entries are stored, defaults to 500 entries
The least recently used entry will be evicted

[[plugins-filters-jdbc_streaming-default_hash]]
===== `default_hash` 

  * Value type is <<hash,hash>>
  * Default value is `{}`

Define a default object to use when lookup fails to return a matching row.
ensure that the key names of this object match the columns from the statement

[[plugins-filters-jdbc_streaming-enable_metric]]
===== `enable_metric` 

  * Value type is <<boolean,boolean>>
  * Default value is `true`

Disable or enable metric logging for this specific plugin instance
by default we record all the metrics we can, but you can disable metrics collection
for a specific plugin.

[[plugins-filters-jdbc_streaming-id]]
===== `id` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

Add a unique `ID` to the plugin configuration. If no ID is specified, Logstash will generate one. 
It is strongly recommended to set this ID in your configuration. This is particularly useful 
when you have two or more plugins of the same type, for example, if you have 2 grok filters. 
Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.

[source,ruby]
---------------------------------------------------------------------------------------------------
output {
 stdout {
   id => "my_plugin_id"
 }
}
---------------------------------------------------------------------------------------------------


[[plugins-filters-jdbc_streaming-jdbc_connection_string]]
===== `jdbc_connection_string` 

  * This is a required setting.
  * Value type is <<string,string>>
  * There is no default value for this setting.

JDBC connection string

[[plugins-filters-jdbc_streaming-jdbc_driver_class]]
===== `jdbc_driver_class` 

  * This is a required setting.
  * Value type is <<string,string>>
  * There is no default value for this setting.

JDBC driver class to load, for example "oracle.jdbc.OracleDriver" or "org.apache.derby.jdbc.ClientDriver"

[[plugins-filters-jdbc_streaming-jdbc_driver_library]]
===== `jdbc_driver_library` 

  * Value type is <<path,path>>
  * There is no default value for this setting.

Tentative of abstracting JDBC logic to a mixin
for potential reuse in other plugins (input/output)
This method is called when someone includes this module
Add these methods to the 'base' given.
JDBC driver library path to third party driver library.

[[plugins-filters-jdbc_streaming-jdbc_password]]
===== `jdbc_password` 

  * Value type is <<password,password>>
  * There is no default value for this setting.

JDBC password

[[plugins-filters-jdbc_streaming-jdbc_user]]
===== `jdbc_user` 

  * Value type is <<string,string>>
  * There is no default value for this setting.

JDBC user

[[plugins-filters-jdbc_streaming-jdbc_validate_connection]]
===== `jdbc_validate_connection` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Connection pool configuration.
Validate connection before use.

[[plugins-filters-jdbc_streaming-jdbc_validation_timeout]]
===== `jdbc_validation_timeout` 

  * Value type is <<number,number>>
  * Default value is `3600`

Connection pool configuration.
How often to validate a connection (in seconds)

[[plugins-filters-jdbc_streaming-parameters]]
===== `parameters` 

  * Value type is <<hash,hash>>
  * Default value is `{}`

Hash of query parameter, for example `{ "id" => "id_field" }`

[[plugins-filters-jdbc_streaming-periodic_flush]]
===== `periodic_flush` 

  * Value type is <<boolean,boolean>>
  * Default value is `false`

Call the filter flush method at regular interval.
Optional.

[[plugins-filters-jdbc_streaming-remove_field]]
===== `remove_field` 

  * Value type is <<array,array>>
  * Default value is `[]`

If this filter is successful, remove arbitrary fields from this event.
Fields names can be dynamic and include parts of the event using the %{field}
Example:
[source,ruby]
    filter {
      jdbc_streaming {
        remove_field => [ "foo_%{somefield}" ]
      }
    }
[source,ruby]
    # You can also remove multiple fields at once:
    filter {
      jdbc_streaming {
        remove_field => [ "foo_%{somefield}", "my_extraneous_field" ]
      }
    }

If the event has field `"somefield" == "hello"` this filter, on success,
would remove the field with name `foo_hello` if it is present. The second
example would remove an additional, non-dynamic field.

[[plugins-filters-jdbc_streaming-remove_tag]]
===== `remove_tag` 

  * Value type is <<array,array>>
  * Default value is `[]`

If this filter is successful, remove arbitrary tags from the event.
Tags can be dynamic and include parts of the event using the `%{field}`
syntax.

Example:
[source,ruby]
    filter {
      jdbc_streaming {
        remove_tag => [ "foo_%{somefield}" ]
      }
    }
[source,ruby]
    # You can also remove multiple tags at once:
    filter {
      jdbc_streaming {
        remove_tag => [ "foo_%{somefield}", "sad_unwanted_tag"]
      }
    }

If the event has field `"somefield" == "hello"` this filter, on success,
would remove the tag `foo_hello` if it is present. The second example
would remove a sad, unwanted tag as well.

[[plugins-filters-jdbc_streaming-statement]]
===== `statement` 

  * This is a required setting.
  * Value type is <<string,string>>
  * There is no default value for this setting.

Statement to execute.
To use parameters, use named parameter syntax, for example "SELECT * FROM MYTABLE WHERE ID = :id"

[[plugins-filters-jdbc_streaming-tag_on_default_use]]
===== `tag_on_default_use` 

  * Value type is <<array,array>>
  * Default value is `["_jdbcstreamingdefaultsused"]`

Append values to the `tags` field if no record was found and default values were used

[[plugins-filters-jdbc_streaming-tag_on_failure]]
===== `tag_on_failure` 

  * Value type is <<array,array>>
  * Default value is `["_jdbcstreamingfailure"]`

Append values to the `tags` field if sql error occured

[[plugins-filters-jdbc_streaming-target]]
===== `target` 

  * This is a required setting.
  * Value type is <<string,string>>
  * There is no default value for this setting.

Define the target field to store the extracted result(s)
Field is overwritten if exists

[[plugins-filters-jdbc_streaming-use_cache]]
===== `use_cache` 

  * Value type is <<boolean,boolean>>
  * Default value is `true`

Enable or disable caching, boolean true or false, defaults to true


